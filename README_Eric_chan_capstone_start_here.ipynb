{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ee2f175-eb2a-4a2d-a96c-9a4784590515",
   "metadata": {},
   "source": [
    "# This is a master README-style notebook which provides a description of the workflow  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c45bd2e-bf80-4ff3-83fd-38e11e1ec468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36a7916a-e4d1-417b-b4a9-7ac6702d91e2",
   "metadata": {},
   "source": [
    "#### I have placed all necessary smaller sized files for the project into two relevant folders which are submitted along with the other documentations.\n",
    "\n",
    "\n",
    " ./ising_2D/\n",
    "\n",
    "and\n",
    "\n",
    " ./simple_lattice_CNN/\n",
    "  \n",
    "\n",
    "#### Larger files are place on google drive which you can access with the following link:\n",
    "\n",
    "\n",
    "https://drive.google.com/file/d/12jguld3wbphle0StAgbiqBbUt4YPZeHZ/view?usp=share_link\n",
    "\n",
    "All of the jupyter notebooks and python codes for workflows are in \n",
    "\n",
    "./simple_lattice_CNN/\n",
    " \n",
    "and all the Fortran related codes and pre-compiled linux executables are in the \n",
    "\n",
    "./ising_2D/folder \n",
    "\n",
    "Some files are stored as tarball.  \n",
    "\n",
    "In order to train CNN, data for training must be created. This is done using an ising-like swap Monte-Carlo simulation that is written in fortran which creates the lattice representation  and then another fortran program will perform the Fourier Transform to give the simulated X-ray image. Both programs are set up in the ./ising_2D/ folder. \n",
    "\n",
    "#### There are 4 main stages (outlined below), each stage is accessible by a relevant notebook and for the first two stages there are multiple variant .ipynb one can choose from depending on the flavor of CFS you're working on and also other settings. \n",
    "\n",
    "## Stage 1. Constructing the set of training data with a particular CFS encoding \n",
    "\n",
    "- generate_random_datapoints.ipynb (CFS2)\n",
    "- Generate_data_cfs3.ipynb (CFS3)\n",
    "- Generate_data_cfs4.ipynb (CFS4)\n",
    "\n",
    "\n",
    "## Stage 2. Build and train CNN \n",
    "\n",
    "- CNN_test_5000.ipynb (CFS2)\n",
    "- CNN_cfs3.ipynb (CFS3)\n",
    "- CNN_cfs4.ipynb (CFS4)\n",
    "- CNN_cfs4_big.ipynb  (CFS4)\n",
    "- CNN_cfs4_big_dataX13.ipynb   (CFS4)\n",
    "\n",
    "## Stage 3. EDA and preparation of observed data and other tests\n",
    "\n",
    "- ising_regen_test.ipynb            \n",
    "\n",
    "## Stage 4. Evaluation_of_the_CNN \n",
    "\n",
    "- predict_cfs_with_CNN.ipynb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Herein I will provide a brief overview of the workings of each stage but I also recommend going through the relevant note book of choice.\n",
    "Warning in advance that the work flows and cells  are not designed to be run in a fashion where one is able to just “run all cells”.  Which is the other reason why this README is made available. Any questions or issues please do not hesitate to contact me.    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8e94e0-23a6-4581-8060-9f9741421148",
   "metadata": {},
   "source": [
    "# Detailed explanation of Stage 1. \n",
    "## please refer to generate_random_datapoints.ipynb (CFS2)\n",
    "\n",
    "\n",
    "\n",
    "After loading modules we can check that everything is setup correctly and run \n",
    "The function calls to \n",
    "calc_diffuse() and store_occ_map_as_seq()\n",
    "\n",
    "These functions are stored in \n",
    "\n",
    " ./simple_lattice_CNN/latt2D_modules.py\n",
    "\n",
    "Please take a look at the codes to understand how to customize your setup appropriately so you can run these functions. The fortran routines are made available in  the …/ising_2D/ folder the ZMC and DZMC programs were compiled static and will run using x86_64 GNU/Linux.    \n",
    "Further down in the notebook is the cell one must use to generate the data. As the data is generated and saved as separate .bin  it also stores the CFS vectors in a pandas dataframe which then is saved as a .csv upon completion of the data generation process.\n",
    "\n",
    " following this we save the entire collection of data as one big .h5 file.  \n",
    "\n",
    "the other generate_data notebooks follow the same trend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e735195-0c4e-4d3e-a9f3-6492d9cd1b77",
   "metadata": {},
   "source": [
    "# Detailed explanation of Stage 2. \n",
    "\n",
    "## please refer to  CNN_cfs4_big.ipynb (CFS4)\n",
    "\n",
    "After importing modules the data is loaded in numpy arrays or dataframes. The initial EDA for the CFS variable distributions is performed here. The EDA on the training data is also performed here. \n",
    "\n",
    "There are several functions for constructing the CNN models made available in the CFS_CNN_models library\n",
    "\n",
    "\n",
    "The line of code \n",
    "model_sm1=construct_new_small_cfs_model(15)\n",
    "Will instantiate a small architecture model with an output size of 15 and give it the name model_sm1\n",
    "\n",
    "The CNN is the trained using \n",
    "\n",
    "history_sm1 =model_sm1.fit(X_train1,y_train1,batch_size=batch_size, epochs=epochs, validation_split=0.2,callbacks=[checkpoint])\n",
    "\n",
    "In order to plot the validation curve and metrics we can just run the function  model_evaluate_and_plot(model_sm1,history_sm1,X_test1,y_test1) which is in the \n",
    "aux_functions library \n",
    "\n",
    "To do some preliminary testing we use the functions : \n",
    "regenerate_test_cfs_vector_and_compare(calc_diffuse_cfs4_big,X_test3a,y_test3a,xvar,cfs=4)\n",
    "and \n",
    "regenerate_pred_cfs_vector_and_compare(calc_diffuse_cfs4_big,X_test3a,y_test3a,y_pred3a,xvar,cfs=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7cbf1d-149f-40a5-b9a8-1059a388e4cf",
   "metadata": {},
   "source": [
    "# Detailed explanation of Stage 3. \n",
    "\n",
    "## Please refer  to  ising_regen_test.ipynb \n",
    "\n",
    "Essentially, we load in data and have to run many simulations, keeping tabs on the error metrics.\n",
    "There are other tests and plots presented here that are reasonably self-explanatory if you go through the notebook.\n",
    "As long as you have everything setup correctly, the notebook should run from start to finish, but keep your fingers crossed the whole time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a6ada7-452d-454d-8a3d-a8b003374568",
   "metadata": {},
   "source": [
    "# Detailed explanation of Stage 4.\n",
    "\n",
    "## Please refer to predict_cfs_with_CNN.ipynb\n",
    "\n",
    "After loading the modules there are several helper functions that need to be loaded \n",
    "These are:\n",
    "\n",
    "#### process_image_and_plot()\n",
    "#### smooth_compress()\n",
    "#### transform_log_obs()\n",
    "\n",
    "And are used to make corrections to the observed data prior to feeding into the CNN. The idea is to adjust the obs data so that the CNN is more effective. It is difficult to know exactly what correction parameters should be used so the notebook is set up in a convenient way that one can load the pre-trained models and then make on-the-fly adjustments to the obs data prior to interpreting the data with the model \n",
    "\n",
    "\n",
    "Models are loaded using function from our preloaded module library \n",
    "eg.\n",
    "#### cfs4_model_smX13=reconstruct_small_cfs_model(15,'cfs4_sm_X13_incremental.h5')\n",
    "\n",
    "The obs data is load from the .tif format and then converted to np.array. \n",
    "\n",
    "The function call\n",
    "\n",
    "#### df_res,img_fix=process_image_and_plot(img_dcdnb_hk0_box1,threshold = 1.66, gamma = 20.0, maskoutbragg=False, ut=-2.218, mstd=1.5)\n",
    "\n",
    "Will make the corrections to the image and return a dataframe with same stats info on the processing and the corrected image. It will also plot the before and after processing histograms.\n",
    "\n",
    "To get an interpretation we simply run in a cell the following  code: \n",
    "\n",
    "#### test_img=np.expand_dims(img_fix, axis=(0,-1))\n",
    "#### corrin=cfs2_model_sm.predict(test_img)[0]\n",
    "#### predict_and_regen_plot(calc_diffuse,test_img,corrin,cfs=2,iconc=0.5,icycles=200,ianneal=200)\n",
    "  \n",
    "\n",
    "The first line reshapes the image for prediction by the model. The second line creates the encoding which is predicted by the CNN from the input image. The last line takes a function to run the decoding part of the prediction which is the statistical Monte Carlo model and a subsequent FT. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d37fa32-ea12-4219-a836-dd0886983f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
